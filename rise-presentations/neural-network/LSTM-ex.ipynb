{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some time series data\n",
    "# 1d time series: each element in the array is a different time point\n",
    "inputs = torch.randn((200,1,1)) # LSTM input dimension is (sequence length, batch size, input size) when batch_first=False, see detials in docs\n",
    "# target is a sliding window mean of the past 'navg' elements\n",
    "targets = torch.zeros((200,1,1))\n",
    "\n",
    "navg = 5\n",
    "for ii in range(navg, inputs.shape[0]):\n",
    "    targets[ii] = inputs[ii - navg:ii].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(1, 10) # input: 1 dim, hidden size: 10 dim\n",
    "        # what's the output size of our LSTM?\n",
    "        self.output_layer = torch.nn.Linear(10, 1) # use a linear layer to project back to the dimension of our data\n",
    "        \n",
    "    def forward(self, x, hc_0):\n",
    "        # hc_0 needs to be a tuple of hidden and cell states, (h_0, c_0)\n",
    "        # hc_n is also a tuple of the final hidden state and cell states\n",
    "        lstm_outputs, hc_n = self.lstm(x, hc_0)\n",
    "        outputs = self.output_layer(lstm_outputs) # plug the output of LSTM as the input to the linear layer\n",
    "        return outputs, hc_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLSTM()\n",
    "criterion = torch.nn.MSELoss() # Why MSE loss? \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss(0): tensor(0.1634, grad_fn=<MeanBackward0>)\n",
      "Loss(10): tensor(0.1175, grad_fn=<MeanBackward0>)\n",
      "Loss(20): tensor(0.0971, grad_fn=<MeanBackward0>)\n",
      "Loss(30): tensor(0.0472, grad_fn=<MeanBackward0>)\n",
      "Loss(40): tensor(0.0240, grad_fn=<MeanBackward0>)\n",
      "Loss(50): tensor(0.0184, grad_fn=<MeanBackward0>)\n",
      "Loss(60): tensor(0.0151, grad_fn=<MeanBackward0>)\n",
      "Loss(70): tensor(0.0140, grad_fn=<MeanBackward0>)\n",
      "Loss(80): tensor(0.0130, grad_fn=<MeanBackward0>)\n",
      "Loss(90): tensor(0.0122, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nepoch = 100\n",
    "\n",
    "hc_0 = (torch.zeros((1,1,10)), torch.zeros((1,1,10))) # (num layer, batch size, hidden size)\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, hc_n = model(inputs, hc_0)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if not epoch%10:\n",
    "        print(\"Loss(%d):\"%epoch, loss.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same input but now batch size = 5\n",
    "inputs_b = torch.randn((200,5,1)) # LSTM input dimension is (sequence length, batch size, input size)\n",
    "targets_b = torch.zeros((200,5,1))\n",
    "\n",
    "navg = 5\n",
    "for ii in range(navg, inputs_b.shape[0]):\n",
    "    targets_b[ii] = inputs_b[ii - navg:ii].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss(0): tensor(0.1213, grad_fn=<MeanBackward0>)\n",
      "Loss(10): tensor(0.0289, grad_fn=<MeanBackward0>)\n",
      "Loss(20): tensor(0.0294, grad_fn=<MeanBackward0>)\n",
      "Loss(30): tensor(0.0285, grad_fn=<MeanBackward0>)\n",
      "Loss(40): tensor(0.0277, grad_fn=<MeanBackward0>)\n",
      "Loss(50): tensor(0.0270, grad_fn=<MeanBackward0>)\n",
      "Loss(60): tensor(0.0266, grad_fn=<MeanBackward0>)\n",
      "Loss(70): tensor(0.0263, grad_fn=<MeanBackward0>)\n",
      "Loss(80): tensor(0.0261, grad_fn=<MeanBackward0>)\n",
      "Loss(90): tensor(0.0260, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we can use the same LSTM because it is set up regardless of batch size\n",
    "nepoch = 100\n",
    "# but we need to set up the hidden states and cell states to have the right dimension! \n",
    "hc_0_b = (torch.zeros((1,5,10)), torch.zeros((1,5,10))) # (num layer, batch size, hidden size)\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    optimizer.zero_grad()\n",
    "    outputs_b, hc_n_b = model(inputs_b, hc_0_b)\n",
    "    loss = criterion(outputs_b, targets_b)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if not epoch%10:\n",
    "        print(\"Loss(%d):\"%epoch, loss.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now 2000 time steps instead of 200! \n",
    "inputs2 = torch.randn((2000,1,1))\n",
    "targets2 = torch.zeros((2000,1,1))\n",
    "\n",
    "navg = 5\n",
    "for ii in range(navg, inputs2.shape[0]):\n",
    "    targets2[ii] = inputs2[ii - navg:ii].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss(0): tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "Loss(1): tensor(0.0478, grad_fn=<MeanBackward0>)\n",
      "Loss(2): tensor(0.0332, grad_fn=<MeanBackward0>)\n",
      "Loss(3): tensor(0.0250, grad_fn=<MeanBackward0>)\n",
      "Loss(4): tensor(0.0213, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nepoch = 5\n",
    "# MyLSTM class doesn't depend on the sequence length, so you can give it variable sequence length!\n",
    "seq_len = 100 # how long should each training sequence be? \n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    data_ptr = 0 # where should we start on this BPTT run?\n",
    "    hc = (torch.zeros((1,1,10)), torch.zeros((1,1,10))) # (num layer, batch size, hidden size)\n",
    "    while True:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hc = model(inputs2[data_ptr:data_ptr+seq_len], (hc[0].detach(),hc[1].detach())) # DETACH!!!\n",
    "        # if do not detach, it'll complain that you've already back propped through the graph, because we're reusing the same h\n",
    "        loss = criterion(outputs, targets2[data_ptr:data_ptr+seq_len])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        data_ptr += seq_len\n",
    "        if data_ptr + seq_len > inputs2.shape[0]:\n",
    "            data_ptr = 0\n",
    "            break # out of the while True loop\n",
    "    \n",
    "    print(\"Loss(%d):\"%epoch, loss.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
